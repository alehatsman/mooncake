# Ollama Ubuntu Demo - With Visible LLM Output

- name: Install Ollama
  shell: curl -fsSL https://ollama.com/install.sh | sh
  timeout: 10m

- name: Verify installation
  shell: ollama --version
  register: version

- name: Start Ollama server
  shell: nohup ollama serve > /tmp/ollama.log 2>&1 & sleep 8

- name: Check server is ready
  shell: |
    for i in {1..10}; do
      if curl -f -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "✓ Ollama server is ready!"
        exit 0
      fi
      sleep 2
    done

- name: Pull tinyllama model
  shell: ollama pull tinyllama
  timeout: 15m

- name: List models
  shell: ollama list

- name: DEMO - Ask LLM a math question
  shell: ollama run tinyllama "What is 2+2? Answer in one short sentence."
  timeout: 3m

- name: DEMO - Ask LLM about geography
  shell: ollama run tinyllama "What is the capital of France? Answer in 3 words only."
  timeout: 3m

- name: DEMO - Ask LLM to write code
  shell: ollama run tinyllama "Write a hello world function in Python. Code only, no explanation."
  timeout: 3m

- name: Display success message
  shell: |
    echo ""
    echo "╔════════════════════════════════════════════════╗"
    echo "║   ✅ OLLAMA UBUNTU DEMO COMPLETE!             ║"
    echo "╚════════════════════════════════════════════════╝"
    echo ""
    echo "Successfully demonstrated:"
    echo "  • Ollama installation in Ubuntu"
    echo "  • Server startup and API health"
    echo "  • Model download (tinyllama ~637MB)"
    echo "  • LLM inference (3 questions answered)"
    echo ""
