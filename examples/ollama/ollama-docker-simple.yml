# Ollama Docker Demo - Simple Version
# Manual Ollama installation for Docker environment

- name: Download Ollama binary from GitHub
  shell: curl -fsSL https://ollama.com/install.sh | sh
  become: true
  register: ollama_install
  timeout: 5m

- name: Verify Ollama binary exists
  shell: which ollama
  register: ollama_path

- name: Show Ollama location
  shell: echo "Ollama installed at{{ ":" }} {{ ollama_path.stdout }}"

- name: Check Ollama version
  shell: ollama --version
  register: ollama_version_output

- name: Show version
  shell: echo "{{ ollama_version_output.stdout }}"

- name: Create Ollama models directory
  file:
    path: /root/.ollama
    state: directory
    mode: "0755"
  become: true

- name: Start Ollama server in background
  shell: nohup ollama serve > /tmp/ollama.log 2>&1 &
  become: true

- name: Wait for Ollama to start (5 seconds)
  shell: sleep 5

- name: Check Ollama API health (with retries)
  shell: curl -f -s http://localhost:11434/api/tags || echo "Not ready"
  register: api_check
  retries: 10
  retry_delay: "2s"
  failed_when: false

- name: Show API check result
  shell: echo "API Status{{ ":" }} {{ api_check.stdout }}"

- name: Pull tinyllama model (small ~637MB model)
  shell: timeout 600 ollama pull tinyllama
  register: model_pull
  timeout: 12m
  become: true

- name: Show model pull result
  shell: echo "{{ model_pull.stdout }}"

- name: List installed models
  shell: ollama list
  register: models_list
  become: true

- name: Show installed models
  shell: |
    echo "======================================"
    echo "Installed Ollama Models:"
    echo "======================================"
    echo "{{ models_list.stdout }}"
    echo "======================================"

- name: Run inference - Ask LLM a question
  shell: timeout 120 ollama run tinyllama "What is 2+2? Answer in one short sentence."
  register: inference_result
  timeout: 3m
  become: true

- name: Display LLM response
  shell: |
    echo ""
    echo "======================================"
    echo "ðŸ¤– LLM Response:"
    echo "======================================"
    echo "{{ inference_result.stdout }}"
    echo "======================================"
    echo ""

- name: Test another question
  shell: timeout 120 ollama run tinyllama "What is the capital of France? Answer in 3 words."
  register: france_result
  timeout: 3m
  become: true

- name: Display second response
  shell: |
    echo "======================================"
    echo "ðŸ¤– Second Query Response:"
    echo "======================================"
    echo "{{ france_result.stdout }}"
    echo "======================================"

- name: Show summary
  shell: |
    echo ""
    echo "âœ… Ollama Demo Complete!"
    echo ""
    echo "Summary:"
    echo "  - Ollama installed and running"
    echo "  - Model: tinyllama (~637MB)"
    echo "  - Successfully ran 2 inference queries"
    echo "  - API endpoint: http://localhost:11434"
