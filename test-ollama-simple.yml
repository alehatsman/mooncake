# Simple Ollama Demo - Install and test with smallest model

- name: install ollama
  preset: ollama
  with:
    state: present
    service: false
    pull: [tinyllama]
  become: true
  register: ollama_install

- name: start ollama server in background
  shell: nohup ollama serve > /tmp/ollama.log 2>&1 &
  become: true

- name: wait for ollama to start
  shell: sleep 3

- name: check ollama api
  shell: curl -s http://localhost:11434/api/tags
  register: api_check
  retries: 5
  retry_delay: 2s

- name: ask ollama a question
  shell: ollama run tinyllama "what is 2+2? answer in one word"
  register: llm_response
  timeout: 1m

- name: show llm response
  shell: |
    echo "================================"
    echo "LLM Response:"
    echo "================================"
    echo "{{ llm_response.stdout }}"
    echo "================================"
